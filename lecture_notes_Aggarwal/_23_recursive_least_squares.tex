\documentclass{beamer}
\input{macros}

\title{ECEn 671: Mathematics of Signals and Systems}
\author{Randal W. Beard}
\institute{Brigham Young University}
\date{\today}

\begin{document}

%-------------------------------
\begin{frame}
	\titlepage
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Batch Least Squares}
\frame{\sectionpage}

%----------------------------------
\begin{frame}\frametitle{Least Squares Filtering Problem}

Suppose that you have an application, like system identification, where you are trying to estimate a set of parameters from noisy data. For example, suppose that you are trying to estimate the parameters of the discrete-time system
\begin{multline*}
y[k] = a_1 y[k-1] + a_2 y[k-2] + \cdots + a_n y[k-n] \\+ b_0 u[k] + b_1 u[k-1] + \cdots + b_m u[k-m]
\end{multline*}
where you know the inputs $u[k]$ and the measure the output $y[k]$ plus noise.

\end{frame}

%----------------------------------
\begin{frame}\frametitle{Least Squares Filtering Problem, cont.}
 
Rewrite the measurement at time $k$ as
\begin{align*}
y[k] &= \begin{pmatrix} y[k-1] & \cdots & y[k-n] & u[k] & \cdots & u[k-m] \end{pmatrix} \begin{pmatrix} a_1 \\ \vdots \\ a_n \\ b_0 \\ \vdots \\ b_{m-1}\end{pmatrix} + \eta \\
 &= \mathbf{a}_k^\top \mathbf{x} + \eta
\end{align*}
where $\eta$ is noise and
\begin{align*}
\mathbf{a}_k^\top &= \begin{pmatrix} y[k-1] & y[k-2] & \cdots & y[k-n] & u[k] & \cdots & u[k-m] \end{pmatrix} \\
\mathbf{x}^\top &= \begin{pmatrix} a_1 & \cdots & a_n & b_0 & \cdots & b_{m-1}\end{pmatrix}.
\end{align*}


\end{frame}

%----------------------------------
\begin{frame}\frametitle{Least Squares Filtering Problem, cont.}

Collecting $N$ samples and stacking as a matrix gives
\begin{align*}
\begin{pmatrix} y[1] \\ \vdots \\ y[N] \end{pmatrix} &= \begin{pmatrix} \mathbf{a}_1^\top \\ \vdots \\ \mathbf{a}_N^\top\end{pmatrix}\mathbf{x}_N + \begin{pmatrix} \eta_1 \\ \vdots \eta_N \end{pmatrix} \\
\implies \mathbf{y}_N &= A_N \mathbf{x}_N + \boldsymbol{\eta}_N,
\end{align*}
where
\begin{align*}
	\mathbf{y}_N &= \begin{pmatrix} y[1] & \cdots & y[N] \end{pmatrix}^\top \\
	A_N &= \begin{pmatrix} \mathbf{a}_1^\top \\ \vdots \\ \mathbf{a}_N^\top\end{pmatrix}
\end{align*}
and $x_N$ is the least squares solution given $N$ samples.

\end{frame}

%----------------------------------
\begin{frame}\frametitle{Least Squares Filtering Problem, cont.}

We know that the batch least squares solution is
\[
\mathbf{x}_N^\ast = \left(A_N^\top A_N\right)^{-1}A_N^\top \mathbf{y}_N
\]

While the matrix $A_N^\top A_N$ is always $(n+m)\times(n+m)$, computing $A_N^\top A_N$ requires the storage and multiplication of matrices of the size $N\times (n+m)$ which can become prohibitively large for a large number of samples.  

\vspace{0.5cm}

Therefore, computing batch least squares at every sample is not a reasonable strategy.

\vspace{0.5cm}

The recursive least squares (RLS) algorithm solves this problem.

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recursive Least Squares Filtering}
\frame{\sectionpage}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering}
Least squares solution:
\[
\xbf_N^\ast = \left(A_N^\top A_N\right)^{-1}A_N^\top \ybf_N
\]

Define
\begin{align*}
	P_N &= \left(A_N^\top A_N\right)^{-1} \\	
	\zbf_N &= A_N^\top \ybf_N
\end{align*}
then
\[
\xbf_N^\ast = \underbrace{P_N}_{(n+m)\times(n+m)} \underbrace{\zbf_N}_{(n+m)\times 1}
\]
where the size of $P_N$ and $\zbf_N$ are independent of the number of samples $N$.
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Note that after $N-1$ samples
	\begin{align*}
		P_{N-1}^{-1} &\defeq A^T_{N-1} A_{N-1} 
		     = \begin{pmatrix}\abf_1 & \cdots & \abf_{t-1}\end{pmatrix} \begin{pmatrix} \abf^T_1 \\ \vdots \\ \abf^T_{t-1} \end{pmatrix} \\
		    &= \sum_{i=1}^{N-1} \abf_i \abf^T_i.
	\end{align*}
	
	\vfill
	
	Receiving a new sample at time $N$: $y[N] = \abf_N^\top \xbf$, then
	
	\vfill
	
	Then 
	\begin{align*}
		P_{N}^{-1} &= \sum_{i=1}^{N} \abf_i \abf^T_i \\
		    &= \sum_{i=1}^{N-1} \abf_i \abf^T_i  + \abf_N \abf_N^T \\
		    &= P_{N-1}^{-1} + \abf_N \abf_N^T.
	\end{align*}
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Using the matrix inversion lemma:
		\[ (A + XRY)^{-1} = A^{-1} - A^{-1}X(R^{-1}+YA^{-1}X)^{-1}YA^{-1} \]
	gives
	\begin{align*}
		P_{N} &= \left(P_{N-1}^{-1} + \abf_N \abf_N^T \right)^{-1} \\
			  &= P_{N-1} - P_{N-1}\abf_N\left(1+\abf_N^\top P_{N-1}\abf_N\right)^{-1}\abf_N^\top P_{N-1} \\
			  &= P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N}
	\end{align*}
	where we note that an $(n+m)\times (n+m)$ inverse has been replaced by a $1\times 1$ inverse.
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Note that we have found a clever way to {\bf recursively} update $P_N = (A_N^\top A_N)^{-1}$ with new data:
	
	\[
	P_{N} = P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N}
	\]
	where $\abf_N$ represents the new data.
	
\end{frame}


%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Similarly
		\begin{align*}
			\zbf_N &= A_N^\top \ybf_N \\
			 	&= \sum_{i=1}^N \abf_i y[i] \\
			 	&= \sum_{i=1}^{N-1} \abf_i y[i] + \abf_N y[N] \\
				&= \zbf_{N-1} + \abf_N y[N]
		\end{align*}
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Therefore the {\bf exact} least squares solution after $N$ samples is 
	\begin{align*}
		\xbf_N	&= (A_N^\top  A_N)^{-1} A_N^\top \ybf_N \\
				&= P_N \zbf_N \\
				&= \left( P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N} \right) \left( \zbf_{N-1} + \abf_N y[N] \right) \\
				&= P_{N-1} \zbf_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N} \zbf_{N-1} \\
				  &\qquad + \left( P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N} \right)\abf_N y[N] \\
				&= \xbf_{N-1} - \left(\frac{P_{N-1}\abf_N}{1+\abf_N^\top P_{N-1}\abf_N}\right) \abf_N^\top P_{N-1}\zbf_{N-1} \\
					&\qquad + \left( P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N} \right) \abf_N y[N] \\
	\end{align*}
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Define (the Kalman gain)
		\[
			\kbf_N \triangleq \frac{P_{N-1}\abf_N}{1+\abf_N^\top P_{N-1}\abf_N}
		\]
	and note that
	\begin{align*}
		&\left(P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N} \right) \abf_N  \\
		&= \frac{P_{N-1}\abf_N(1+\abf_N^\top P_{N-1}\abf_N) - P_{N-1}\abf_N\abf_N^\top P_{N-1}\abf_N}{1+\abf_N^\top P_{N-1}\abf_N}  \\
		&= \frac{P_{N-1}\abf_N+ P_{N-1}\abf_N\abf_N^\top P_{N-1}\abf_N - P_{N-1}\abf_N\abf_N^\top P_{N-1}\abf_N}{1+\abf_N^\top P_{N-1}\abf_N}  \\
		&= \frac{P_{N-1}\abf_N}{1+\abf_N^\top P_{N-1}\abf_N}  \\
		&= \kbf_N
	\end{align*}
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, cont.}
	Therefore
	\begin{align*}
		\xbf_N	&= \xbf_{N-1} - \left(\frac{P_{N-1}\abf_N}{1+\abf_N^\top P_{N-1}\abf_N}\right) \abf_N^\top P_{N-1}\zbf_{N-1} \\
					&\qquad + \left( P_{N-1} - \frac{P_{N-1}\abf_N\abf_N^\top P_{N-1}}{1+\abf_N^\top P_{N-1}\abf_N} \right) \abf_N y[N] \\
				&= \xbf_{N-1} - \kbf_N \abf_N^\top P_{N-1}\zbf_{N-1} + \kbf_N y[N] \\
				&= \xbf_{N-1} + \kbf_N\left( y[N] - \abf_N^\top P_{N-1}\zbf_{N-1}\right) \\
				&= \xbf_{N-1} + \kbf_N\left( y[N] - \abf_N^\top \xbf_{N-1}\right) \\
	\end{align*}

	Note that $\hat{y}[N] = \abf_N^\top \xbf_{N-1}$ is the predicted output, and $e_N=y[N]-\hat{y}[N]$ is the quantity that is being minimized.
\end{frame}

%----------------------------------
\begin{frame}\frametitle{Recursive Least Squares Filtering, interpretation.}
	\[
	\underbrace{\xbf_N}_{\text{new estimate}} = \underbrace{\xbf_{N-1}}_{\text{old estimate}}
		+ \underbrace{\kbf_N}_{\text{Kalman gain}} \underbrace{(y[N]-\hat{y}[N])}_{\text{innovation}}
	\]
	where the innovation is the difference between the actual measurement and the predicted measurement.

\end{frame}


%----------------------------------
\begin{frame}\frametitle{Summary: Recursive Least Squares Filtering}
	At time $t=0$ initialize algorithm with
	\begin{align*}
		P_0 &= \alpha I, \text{~where $\alpha>0$ is a large number} \\
		\xbf_0 &= 0.
	\end{align*}
	At sample $N$, collect output $y[N]$ and input $u[N]$ and construct $\abf_N$ from using current and past inputs and outputs.  
	
	\vfill 
	
	Update the least squares estimate using  
	\begin{align*}
  		\kbf_N &= \frac{P_{N-1} \abf_N}{1 + \abf_t^\top P_{N-1} \abf_N} \\
  		P_N &= P_{N-1} - \kbf_N \abf_N^\top P_{N-1} \\
  		\xbf_N &= \xbf_{N - 1} + \kbf_N (y[N] - \abf_N^\top \xbf_{N-1}).
	\end{align*}
This is equivalent to a discrete time Kalman filter with stationary dynamics.
	
\end{frame}




\end{document}